---
title: "car market research"
author: "Wenwei Fei"
date: "28/08/2021"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r packages}
library(tidyverse)
library(RSelenium)
library(rvest)
library(xml2)
library(httr)
library(data.table)
```

```{r server pack}
# Check the available chrome version, the default is latest
verlist <- binman::list_versions("chromedriver") %>% flatten_chr()
# check my chrome version. wmic: WMI command-line 
version <- system2(command = "wmic",args = 'datafile where name="C:\\\\Program Files (x86)\\\\Google\\\\Chrome\\\\Application\\\\chrome.exe" get Version /value',stdout = TRUE,stderr = TRUE)
version <- str_sub(version[3],start = 9,end = -2)
# select the latest version earlier the current chrome version 
ver <- max(verlist[version > verlist])
# driver setting
driver <- rsDriver(browser = "chrome", chromever = ver)
driver$client$close()
redr <- driver[["client"]]
redr$open()
# redr$close()
```

```{r}
source("D:/Projects/Scrape functions/driver open.R")
open_driver("yes")
source("D:/Projects/Scrape functions/driver kill.R")
driver_kill("yes")
```

```{r port kill}
# to cleanup the port, by kill the java instance(s) inside Rstudio
# refer: https://github.com/ropensci/RSelenium/issues/228
system("taskkill /im java.exe /f", intern=FALSE, ignore.stdout=FALSE)
# check if ports still open
pingr::ping_port("localhost", 4567)
```

```{r}
kill_port <- function(action){
  if(action = "TRUE"){
    system("taskkill /im java.exe /f", intern=FALSE, ignore.stdout=FALSE)
    pingr::ping_port("localhost", 4567)
  }
  if(action = "FALSE"){print("No action take")}
}
```

```{r test}
url <- "https://www.carsales.com.au/cars/audi/?sort=~Price"
redr$navigate(url)

# rvest failed
```

```{r selenium rvest test}
# titles
title <- redr$findElements("xpath", value = '//*[@id="SSE-AD-7169755"]')
title <- redr$findElement("xpath", value = '//*[@id="SSE-AD-7169755"]')

soup <- redr$getPageSource()
page <- xml2::read_html(soup[[1]])
odome <- page %>% html_nodes(".key-details__value:nth-child(1)") %>% html_text()
car_type <- page %>% html_nodes(".key-details__value:nth-child(2)") %>% html_text()
Trans <- page %>% html_nodes(".key-details__value:nth-child(3)") %>% html_text()
engin <- page %>% html_nodes(".key-details__value:nth-child(4)") %>% html_text()
subadd <- page %>% html_nodes(".card-body .col .js-encode-search") %>% html_attr("href")
```

```{r}
# subaddress test
url2 <- "https://www.carsales.com.au/cars/details/2001-audi-a3-auto-my01/SSE-AD-7169755/?Cr=0"
redr$navigate(url2)
soup2 <- redr$getPageSource()
page <- xml2::read_html(soup2[[1]])
price <- page %>% html_nodes(".price") %>% html_text()

remove <- c("\\$", "\\,","\\*")
gsub("\\$|\\,|\\*","",price) %>% as.numeric()
```

```{r variables}
base_url <- "https://www.carsales.com.au"
brand <- c("audi")
subdir <- "/cars/"
filt_page <- "/?sort=~Price&offset="
# for audi 5820 cars, 485 pages

body_type <- c("Cab Chassis", "Hatch", "Sedan", "SUV", "Ute", "Coupe", "Wagon")
new_used <- c("used","new","demo-and-near-new")
popular_brand <- c("BMW","Holden","Hyundai","Jeep","Land Rover", "Mazda","Mitsubishi", "Nissan","Subaru","Toyota","Isuzu", "Mercedes-Benz", "Volkswagen")

###########
# filters #
###########
years <- seq(2011,2021,1)
page_order <- seq(0,485,1)*12
popular_make <- tolower(popular_brand) %>% gsub("\\ ","-",.)
# "https://www.carsales.com.au/cars/[2021]/[toyota]/?sort=~Price&offset=[12]"
#                                   year    make                        page
```


```{r}
#######
# urls#
#######
# use the following to collect number of cars and decide the number of page. The maximum pages for each filtered results is 83. Apply a small number of filters to ensure the search result contains lesser than 996 cars. 

urls_years <- sapply(years, function(x){
  url <- "https://www.carsales.com.au/cars/"
  urls <- paste0(url,x,"/")
})

urls_years_make <- sapply(popular_make, function(x){
  url <- urls_years
  urls <- paste0(url,x,"/?sort=~Price&offset=0")
})
urls_y_m <- urls_years_make[1:143]

redr$open()
redr$navigate(urls_y_m[1])

# sample test to extract number of cars
redr$navigate(urls_y_m[1])
source <- redr$getPageSource()
page <- xml2::read_html(source[[1]])
number_car <- page %>% html_nodes(".no-gutters .title") %>% html_text() %>% word(.,1,1) %>% gsub("\\,","",.) %>% as.numeric(.)
number_page <- number_car/12
number_page <- floor(number_page)
seq(0,number_page,1)*12

###########################
# apply method to all urls
###########################
store_number <- data.frame()
get_pagenumber <- function(urls){
  redr$navigate(urls)
  Sys.sleep(0.1)
  source <- redr$getPageSource()
  page <- xml2::read_html(source[[1]])
  number_car <- page %>% html_nodes(".no-gutters .title") %>% html_text() %>% word(.,1,1) %>% gsub("\\,","",.) %>% as.numeric(.)
  print(number_car)
  store_number <<- rbind(store_number,number_car)
}
# because of the anti-robot built in carsales, it is better to seperate to 30 units a group. Maybe longer sys.sleep() can trick the anti-robot? worth to try. 
sapply(urls_y_m[118:143], get_pagenumber)

store_number <- store_number$X250[1:69]
store_number <- store_number$number_cars[1:117]
store_number <- data.frame(number_cars = store_number)
###############
# make sequence
###############

store_number <- data.table(store_number)
store_number[, pages := floor(store_number$number_cars/12)]

##########################
# urls without page number
##########################
urls_y <- sapply(years, function(x){
  url <- "https://www.carsales.com.au/cars/"
  urls <- paste0(url,x,"/")
})

urls_ym <- sapply(popular_make, function(x){
  url <- urls_y
  urls <- paste0(url,x,"/?sort=~Price&offset=")
})

urls <- urls_ym[1:143]

# change the number that over 83 to 83, will lost some data
page_numbers <- store_number$pages
page_number <- if_else(page_numbers >= 83, 83, page_numbers)

# make sequence strings for page variables
page_number <- sapply(page_number, function(x){
  pages <- seq(0,x,1)*12
})


t(page_number[[1]])
test_urls <- data.frame()
#####################################
# sample test
#############
sapply(page_number[[2]], function(sequ){
  url <- urls[2]
  urls <- paste0(url,sequ)
  test_urls <<- rbind(test_urls, urls)
})
######################################

#######################################
test_urls <- data.frame()
for (i in 1:143) {
  sapply(page_number[[i]], function(x){
    url <- urls[i]
    urlss <- paste0(url,x)
    test_urls <<- rbind(test_urls,urlss)
  })
  
}
#########################################
##################
# tailor the list
##################
test_urls <- test_urls$X.https...www.carsales.com.au.cars.2011.bmw..sort..Price.offset.0.
test_urls <- data.frame(urls = test_urls)
test_urls <- test_urls[,1]
```


```{r variable capture test}
redr$navigate("https://www.carsales.com.au/cars/2021/hyundai/?sort=~Price&offset=552")
source <- redr$getPageSource()
page <- xml2::read_html(source[[1]])
# test_odom <- page %>% html_nodes("") %>% html_text()
###############
# Variables
###############
# year, make, model, badge in the title
# what needs to be captured are:
# title, price*, odometer, body_type, trans_type, engine_type, state

#########
# title #
#########
test_title <- page %>% html_nodes(".card-body .col .js-encode-search") %>% html_text()
###########
# PRICE*
###########
test_price <- page %>% html_nodes(".price .js-encode-search") %>% html_text() %>% gsub("\\$|\\*|\\,", "",.) %>% as.numeric()
###########
# odometer
###########
test_odom <- page %>% html_nodes(".key-details__value:nth-child(1)") %>% html_text()
###########
# Body type
############
test_body <- page %>% html_nodes(".key-details__value:nth-child(2)") %>% html_text()
###############
# transmission
###############
test_trans <- page %>% html_nodes(".key-details__value:nth-child(3)") %>% html_text()
################
# engine
###############
test_engine <- page %>% html_nodes(".key-details__value:nth-child(4)") %>% html_text()
###############
# state
###############
test_state <- page %>% html_nodes(".seller-location") %>% html_text()
################
# seller
################
test_seller <- page %>% html_nodes(".seller-type") %>% html_text()
###############
# put into df
################
test_dataframe <- data.frame(title=test_title, price = test_price, odometer = test_odom, body = test_body, trans = test_trans, engline = test_engine, state = test_state)
```

```{r apply method to all links}
df_all <- data.table()
df_urls <- data.table(urls = test_urls)
links <- sapply(test_urls, as.character)
redr$navigate(links[22])
## Embed redr$navigate() into a function failed
scraper <- function(address){
  # give value
  #url_inloop <- links
  # page source and parse
  #redr$navigate(url_inloop)
  #Sys.sleep(1)
  #source_inloop <- redr$getPageSource()
  #page_inloop <- xml2::read_html(source_inloop[[1]])
  
  # variables
  ## title
  title_il <- page_inloop %>% html_nodes(".card-body .col .js-encode-search") %>% html_text()
  ## price
  price_il <- page_inloop %>% html_nodes(".price .js-encode-search") %>% html_text() %>% gsub("\\$|\\*|\\,", "",.) %>% as.numeric()
  ## odometer
  odom_il <- page_inloop %>% html_nodes(".key-details__value:nth-child(1)") %>% html_text()
  ## body
  body_il <- page_inloop %>% html_nodes(".key-details__value:nth-child(2)") %>% html_text()
  ## trans
  trans_il <- page_inloop %>% html_nodes(".key-details__value:nth-child(3)") %>% html_text()
  ## engine
  engine_il <- page_inloop %>% html_nodes(".key-details__value:nth-child(4)") %>% html_text()
  ## state
  state_il <- page_inloop %>% html_nodes(".seller-location") %>% html_text() %>% word(.,1,1) %>% gsub("\\n","",.)
  ## seller
  seller_il <- page %>% html_nodes(".seller-type") %>% html_text() %>% gsub("\\n","",.)
  ## df
  df_12_il <- data.frame(title = title_il, price = price_il, odometer = odom_il, body = body_il, trans = trans_il, engline = engine_il, state = state_il, seller = seller_il)
  ## merge
  df_all <<- rbind(df_all,df_12_il)
}
```


```{r apply method to all links}
## unable to perform via function I built, so I tried for loop. The reason may because of the redr$getPageSource() which is not capable to run in funciton.

# first #### pages
for (i in 6149:6417) {
  url_inloop <- links[i]
  redr$navigate(url_inloop)
  Sys.sleep(1.1)
  source_inloop <- redr$getPageSource()
  page_inloop <- xml2::read_html(source_inloop[[1]])
  # variables
  ## title
  title_il <- page_inloop %>% html_nodes(".card-body .col .js-encode-search") %>% html_text()
  ## price
  price_il <- page_inloop %>% html_nodes(".price .js-encode-search") %>% html_text()
  ## odometer
  odom_il <- page_inloop %>% html_nodes(".key-details__value:nth-child(1)") %>% html_text()
  ## body
  body_il <- page_inloop %>% html_nodes(".key-details__value:nth-child(2)") %>% html_text()
  ## trans
  trans_il <- page_inloop %>% html_nodes(".key-details__value:nth-child(3)") %>% html_text()
  ## engine
  engine_il <- page_inloop %>% html_nodes(".key-details__value:nth-child(4)") %>% html_text()
  ## state
  state_il <- page_inloop %>% html_nodes(".seller-location") %>% html_text()
  state_2_il <- page_inloop %>% html_nodes(".franchise-name") %>% html_text()
  ## seller
  seller_il <- page_inloop %>% html_nodes(".seller-type") %>% html_text()
  ## df
  df_12_il <- data.table(title = title_il, price = price_il, odometer = odom_il, body = body_il, trans = trans_il, engline = engine_il, state = state_il, seller = seller_il, state_2 = state_2_il)
  ## merge
  df_all <<- rbind(df_all,df_12_il)
  df_12_il <- data.table()
  print(i)
}
## Big mistake made here! seller_il was given by value page rather than page_inloop, which affected over 50% of relevant data, consider to delete/ignore the seller. 
## re-scrape this variable would be time consuming because some vehicle might sold yet.
## re-correct seller variable from #47999
```


```{r apply method to all links}
df_all_fixed <- df_all[1:76819,]
df_all_fixed[, state_2 := state]
df_all <- df_all_fixed

df_all <- df_all[1:59984,]

fwrite(df_all_fixed, "D:/Projects/EVE/Eve/data_collect_raw_08_30_2021.csv")
fwrite(df_urls, "D:/Projects/EVE/Eve/urls.csv")
df_all <- fread("D:/Projects/EVE/Eve/data_collect_raw_08_29_2021.csv")
```






























